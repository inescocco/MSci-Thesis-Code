{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9466a5",
   "metadata": {},
   "source": [
    "# Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "from collections import Counter\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "#from pytorch_grad_cam import GradCAM\n",
    "#from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "#from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization for training\n",
    "data_transforms = {\n",
    "    'Train_sorted': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'Validation_sorted': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'Test_sorted': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "}\n",
    "\n",
    "data_dir = '/Users/inescocco/Desktop/ISIC2019'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['Train_sorted', 'Validation_sorted']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['Train_sorted', 'Validation_sorted']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['Train_sorted', 'Validation_sorted']}\n",
    "class_names = image_datasets['Train_sorted'].classes\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eea7b",
   "metadata": {},
   "source": [
    "# Unweighted Hard Voting Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5253c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_accuracy(model_path, val_dir, class_names, device):\n",
    "    model = torch.load(model_path, weights_only = False)  # No need to load state_dict separately\n",
    "    model = model.to(device)  # Move model to the device\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterating through each image in the validation directory\n",
    "    for class_idx, class_name in tqdm(enumerate(class_names), desc=\"Processing classes\", total=len(class_names)):\n",
    "        class_folder = os.path.join(val_dir, class_name)\n",
    "        \n",
    "        # Iterating through each image file in the class folder\n",
    "        for filename in os.listdir(class_folder):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):  # Or any image extension\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "                # Applying the necessary transformations\n",
    "                data_transforms = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                img_tensor = data_transforms(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Sending image tensor to the correct device\n",
    "                img_tensor = img_tensor.to(device)\n",
    "\n",
    "                # Getting predictions\n",
    "                outputs = model(img_tensor)\n",
    "                _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
    "\n",
    "                # Updating correct and total counts\n",
    "                total += 1\n",
    "                if preds.item() == class_idx:  # Compare predicted class to the true class index\n",
    "                    correct += 1\n",
    "\n",
    "    accuracy = correct / total  # Calculate accuracy\n",
    "    return accuracy\n",
    "\n",
    "def output_validation_accuracy(model_paths):\n",
    "    validation_accuracies = []\n",
    "    for model_path in model_paths:\n",
    "        accuracy = get_validation_accuracy(model_path, val_dir, class_names, device)\n",
    "        #print(accuracy)\n",
    "        validation_accuracies.append(accuracy)\n",
    "        \n",
    "    return validation_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aa80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_paths, device):\n",
    "    models = []\n",
    "    for model in model_paths:\n",
    "        model = torch.load(model, weights_only = False)\n",
    "        model = model.to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def hard_ensemble(models, validation_accuracies, val_dir, class_names):\n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    all_results = []  # Store predictions for all images\n",
    "\n",
    "    # Iterating through each image in the validation directory for prediction\n",
    "    for class_idx, class_name in tqdm(enumerate(class_names), desc=\"Processing classes\", total=len(class_names)):\n",
    "        class_folder = os.path.join(val_dir, class_name)\n",
    "\n",
    "        for filename in os.listdir(class_folder):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "                # Applying the necessary transformations\n",
    "                data_transforms = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                img_tensor = data_transforms(img).unsqueeze(0)  # Add batch dimension\n",
    "                img_tensor = img_tensor.to(device)\n",
    "\n",
    "                # Getting predictions from each model\n",
    "                model_preds = []\n",
    "                for model in models:\n",
    "                    outputs = model(img_tensor)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    model_preds.append(preds.item())\n",
    "\n",
    "                # Applying majority voting and tie-breaking if needed\n",
    "                vote_count = Counter(model_preds)\n",
    "                if len(vote_count) == 1:  # No tie, just pick the class\n",
    "                    final_pred = vote_count.most_common(1)[0][0]\n",
    "                elif len(vote_count) == 2:  # Two predictions agree, take majority\n",
    "                    final_pred = vote_count.most_common(1)[0][0]\n",
    "                else:\n",
    "                    # If all predictions are different, break the tie using the model with highest validation accuracy\n",
    "                    final_pred = model_preds[np.argmax([validation_accuracies])]\n",
    "\n",
    "                # Storing the results: file name, model predictions, ensemble prediction, and true label\n",
    "                true_label = class_idx\n",
    "                image_file = image_path\n",
    "                model_preds_named = [class_names[pred] for pred in model_preds]\n",
    "                final_pred_named = class_names[final_pred]\n",
    "                true_label_named = class_names[true_label]\n",
    "\n",
    "                all_results.append((image_file, model_preds_named, final_pred_named, true_label_named))\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Function to print predictions and results\n",
    "def hard_print_predictions(results, class_names):\n",
    "    for i, (image_file, model_preds, ensemble_pred, true_label) in enumerate(results):\n",
    "        print(f\"Image {i + 1}: {image_file}\")\n",
    "        \n",
    "        # Printing the true label (numeric and string)\n",
    "        true_label_index = class_names.index(true_label)  # Convert string label to numeric index\n",
    "        print(f\"  True Label: {true_label}, {true_label_index}\")\n",
    "        \n",
    "        # Printing model predictions (both class names and numeric labels)\n",
    "        print(f\"  Model Predictions: {[(pred, class_names.index(pred)) for pred in model_preds]}\")\n",
    "        \n",
    "        # Printing ensemble prediction (both class name and numeric label)\n",
    "        ensemble_pred_index = class_names.index(ensemble_pred)  # Get the numeric index of ensemble prediction\n",
    "        print(f\"  Ensemble Prediction: {ensemble_pred}, {ensemble_pred_index}\\n\")\n",
    "\n",
    "def calculate_ensemble_accuracy(results, class_names):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the ensemble predictions.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of tuples containing (image_file, model_preds, ensemble_pred, true_label).\n",
    "        class_names (list): List of class names.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the ensemble predictions as a percentage.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for _, _, ensemble_pred, true_label in results:\n",
    "        # Converting the string labels to numeric indices for comparison\n",
    "        ensemble_pred_index = class_names.index(ensemble_pred)\n",
    "        true_label_index = class_names.index(true_label)\n",
    "\n",
    "        # Checking if the prediction matches the true label\n",
    "        if ensemble_pred_index == true_label_index:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    # Calculating accuracy as a percentage\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = \"model_b7_2_epoch_7_3.pth\"\n",
    "best_model = torch.load(best_model, weights_only = False)\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "model_paths = [\"model_b7_2_epoch_7_3.pth\",\n",
    "\"model_b6_epoch_15_3.pth\",\n",
    "\"model_b5_epoch_14.pth\"]\n",
    "val_dir = '/Users/inescocco/Desktop/ISIC2019/Test_sorted'\n",
    "validation_accuracies = [0.8008, 0.7434, 0.7424]\n",
    "models = load_models(model_paths, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b7a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running ensemble voting and print results\n",
    "hard_results = hard_ensemble(models, validation_accuracies, val_dir, class_names)\n",
    "hard_print_predictions(hard_results, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf79d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_accuracy = calculate_ensemble_accuracy(hard_results, class_names)\n",
    "print(f\"Unweighted Hard Voting Ensemble Accuracy: {ensemble_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9c65b",
   "metadata": {},
   "source": [
    "# Weighted Hard Voting Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfa41e-890c-4d29-9e5b-714f639dee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies = [0.8008, 0.7434, 0.7424]\n",
    "# CODE ADOPTED FROM: https://www.geeksforgeeks.org/how-to-normalize-an-array-in-numpy-in-python/\n",
    "\n",
    "# explicit function to normalize array\n",
    "def normalize(arr, t_min, t_max):\n",
    "\tnorm_arr = []\n",
    "\tdiff = t_max - t_min\n",
    "\tdiff_arr = max(arr) - min(arr) \n",
    "\tfor i in arr:\n",
    "\t\ttemp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
    "\t\tnorm_arr.append(temp)\n",
    "\treturn norm_arr\n",
    "\n",
    "# gives range starting from 1 and ending at 3 \n",
    "array_1d = val_accuracies \n",
    "range_to_normalize = (0,1)\n",
    "normalized_array_1d = normalize(array_1d, \n",
    "\t\t\t\t\t\t\t\trange_to_normalize[0], \n",
    "\t\t\t\t\t\t\t\trange_to_normalize[1])\n",
    "\n",
    "# display original and normalized array\n",
    "print(\"Original Array = \",array_1d)\n",
    "print(\"Normalized Array = \",normalized_array_1d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_ensemble_weighted(models, val_accuracies, val_dir, class_names):\n",
    "    \"\"\"\n",
    "    Perform ensemble voting with weighted majority class votes.\n",
    "\n",
    "    Args:\n",
    "        model_paths: List of paths to the model files.\n",
    "        val_accuracies: List of validation accuracies for the models.\n",
    "        val_dir: Path to the validation directory.\n",
    "        class_names: List of class names.\n",
    "        device (torch.device): Device to run models.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples containing (image_file, model_preds, ensemble_pred, true_label).\n",
    "    \"\"\"\n",
    "\n",
    "    weights = normalize_array_1d\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    all_results = []  # Store predictions for all images\n",
    "\n",
    "    # Iterating through each image in the test directory for prediction\n",
    "    for class_idx, class_name in tqdm(enumerate(class_names), desc=\"Processing classes\", total=len(class_names)):\n",
    "        class_folder = os.path.join(val_dir, class_name)\n",
    "\n",
    "        for filename in os.listdir(class_folder):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "                # Applying the necessary transformations\n",
    "                img_tensor = data_transforms(img).unsqueeze(0)  # Add batch dimension\n",
    "                img_tensor = img_tensor.to(device)\n",
    "\n",
    "                # Getting predictions from each model\n",
    "                model_preds = []\n",
    "                for model in models:\n",
    "                    outputs = model(img_tensor)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    model_preds.append(preds.item())\n",
    "\n",
    "                # Calculating weighted votes\n",
    "                weighted_votes = np.zeros(len(class_names))\n",
    "                for i, pred in enumerate(model_preds):\n",
    "                    weighted_votes[pred] += weights[i]\n",
    "\n",
    "                # Determining the ensemble prediction based on weighted votes\n",
    "                final_pred = np.argmax(weighted_votes)\n",
    "\n",
    "                # Storing results: file name, model predictions, ensemble prediction, and true label\n",
    "                true_label = class_idx\n",
    "                image_file = image_path\n",
    "                model_preds_named = [class_names[pred] for pred in model_preds]\n",
    "                final_pred_named = class_names[final_pred]\n",
    "                true_label_named = class_names[true_label]\n",
    "\n",
    "                all_results.append((image_file, model_preds_named, final_pred_named, true_label_named))\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972729b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running ensemble voting and print results for weighted ensemble predictions\n",
    "hard_results_weighted = hard_ensemble_weighted(models, validation_accuracies, val_dir, class_names)\n",
    "hard_print_predictions(hard_results_weighted, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b44df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_accuracy_weights = calculate_ensemble_accuracy(hard_results_weighted, class_names)\n",
    "print(f\"Weighted Hard Voting Ensemble Accuracy: {ensemble_accuracy_weights:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0c2a5",
   "metadata": {},
   "source": [
    "# Unweighted Soft Voting Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_ensemble(models, val_dir, class_names, device):\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    all_results = []  # Store predictions for all images\n",
    "    \n",
    "    # Iterating through each class in the validation directory\n",
    "    for class_idx, class_name in tqdm(enumerate(class_names), desc=\"Processing classes\", total=len(class_names)):\n",
    "        class_folder = os.path.join(val_dir, class_name)\n",
    "\n",
    "        # Iterating through each image in the class folder for prediction\n",
    "        for filename in os.listdir(class_folder):\n",
    "            if filename.endswith('.jpg'):\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "                # Applying the necessary transformations\n",
    "                img_tensor = data_transforms(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "                # Initialising an empty list to store the model predictions (scores)\n",
    "                model_scores = []\n",
    "                \n",
    "                # Getting predictions (raw scores or probabilities) from each model\n",
    "                for model in models:\n",
    "                    model.eval()  # Set the model to evaluation mode\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(img_tensor)  # Get the raw outputs from the model\n",
    "                        model_scores.append(outputs.cpu().numpy())  # Store the model output\n",
    "\n",
    "                # Converting model scores to a NumPy array and compute the average across all models\n",
    "                model_scores = np.array(model_scores)  # Shape: [num_models, num_classes]\n",
    "                avg_scores = np.mean(model_scores, axis=0)  # Compute the average across models for each class\n",
    "                \n",
    "                # Predicting the class with the highest average score\n",
    "                final_pred = np.argmax(avg_scores)  # Get the index of the highest average score\n",
    "\n",
    "                # Storing the result: image file, model scores, ensemble prediction, and true label\n",
    "                true_label = class_idx\n",
    "                image_file = image_path\n",
    "        \n",
    "                # Formatting each class score individually by iterating over the avg_scores array\n",
    "                final_pred_named = class_names[final_pred]\n",
    "                true_label_named = class_names[true_label]\n",
    "\n",
    "                # Appending the results for this image\n",
    "                all_results.append((image_file, avg_scores, final_pred_named, true_label_named))\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def soft_print_predictions(results, class_names):\n",
    "    # Iterating through the results\n",
    "    for image_file, avg_scores, ensemble_pred_named, true_label_named in results:\n",
    "        # Extracting true label index (class name to index)\n",
    "        true_label_index = class_names.index(true_label_named)\n",
    "        \n",
    "        # Printing image info\n",
    "        print(f\"Image: {image_file}\")\n",
    "        print(f\"  True Label: {true_label_named}, {true_label_index}\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        print(f\"Averages: {avg_scores}\")\n",
    "    \n",
    "        # Printing ensemble prediction (both class name and numeric label)\n",
    "        ensemble_pred_index = class_names.index(ensemble_pred_named)  # Get the numeric index of ensemble prediction\n",
    "        print(f\"  Ensemble Prediction: {ensemble_pred_named}, {ensemble_pred_index}\")\n",
    "        print(\"-\" * 50)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e23978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running ensemble voting and print results for weighted ensemble predictions\n",
    "soft_results = soft_ensemble(models, val_dir, class_names, device)\n",
    "soft_print_predictions(soft_results, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b20f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_accuracy = calculate_ensemble_accuracy(soft_results, class_names)\n",
    "print(f\"Unweighted Soft Voting Ensemble Accuracy: {ensemble_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ec86e",
   "metadata": {},
   "source": [
    "# Weighted Soft Voting Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986845e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_ensemble_weighted(models, val_dir, class_names, device, val_accuracies):\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    all_results = []  # Store predictions for all images\n",
    "    model_weights = [w / sum(val_accuracies) for w in val_accuracies]  # Normalize to sum to 1\n",
    "\n",
    "    \n",
    "    # Iterating through each class in the validation directory\n",
    "    for class_idx, class_name in tqdm (enumerate(class_names), total=len(class_names), desc=\"Processing Classes\"):\n",
    "        class_folder = os.path.join(val_dir, class_name)\n",
    "\n",
    "        # Iterating through each image in the class folder for prediction\n",
    "        for filename in os.listdir(class_folder):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "                # Applying the necessary transformations\n",
    "                img_tensor = data_transforms(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "                # Initialising an empty list to store the model predictions (scores)\n",
    "                model_scores = []\n",
    "                \n",
    "                # Getting predictions (raw scores or probabilities) from each model\n",
    "                for model in models:\n",
    "                    model.eval()  # Set the model to evaluation mode\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(img_tensor)  # Get the raw outputs from the model\n",
    "                        model_scores.append(outputs.cpu().numpy())  # Store the model output\n",
    "\n",
    "                # Converting model scores to a NumPy array\n",
    "                model_scores = np.array(model_scores)  # Shape: [num_models, num_classes]\n",
    "\n",
    "                # Computing the weighted average across models for each class\n",
    "                avg_scores = np.average(model_scores, axis=0, weights=model_weights)  # Weighted average\n",
    "                \n",
    "                # Predicting the class with the highest average score\n",
    "                final_pred = np.argmax(avg_scores)  # Get the index of the highest average score\n",
    "\n",
    "                # Storing the result: image file, model scores, ensemble prediction, and true label\n",
    "                true_label = class_idx\n",
    "                image_file = image_path\n",
    "        \n",
    "                # Formatting each class score individually by iterating over the avg_scores array\n",
    "                final_pred_named = class_names[final_pred]\n",
    "                true_label_named = class_names[true_label]\n",
    "\n",
    "                # Appending the results for this image\n",
    "                all_results.append((image_file, avg_scores, final_pred_named, true_label_named))\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def soft_print_predictions(results, class_names):\n",
    "    # Iterating through the results\n",
    "    for image_file, avg_scores, ensemble_pred_named, true_label_named in tqdm(results, desc=\"Printing Predictions\"):\n",
    "        # Extracting true label index (class name to index)\n",
    "        true_label_index = class_names.index(true_label_named)\n",
    "        \n",
    "        # Printing image info\n",
    "        print(f\"Image: {image_file}\")\n",
    "        print(f\"  True Label: {true_label_named}, {true_label_index}\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        print(f\"Averages: {avg_scores}\")\n",
    "    \n",
    "    \n",
    "        # Printing ensemble prediction (both class name and numeric label)\n",
    "        ensemble_pred_index = class_names.index(ensemble_pred_named)  # Get the numeric index of ensemble prediction\n",
    "        print(f\"  Ensemble Prediction: {ensemble_pred_named}, {ensemble_pred_index}\")\n",
    "        print(\"-\" * 50)  # Separator for clarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444c8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running ensemble voting and print results for weighted ensemble predictions\n",
    "soft_results_weighted = soft_ensemble_weighted(models, val_dir, class_names, device, validation_accuracies)\n",
    "soft_print_predictions(soft_results_weighted, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_accuracy = calculate_ensemble_accuracy(soft_results_weighted, class_names)\n",
    "print(f\"Weighted Soft Voting Ensemble Accuracy: {ensemble_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f773f",
   "metadata": {},
   "source": [
    "## Comparing Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed467deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functioning to calculate and print evaluation metrics\n",
    "def evaluate_ensemble(results, class_names):\n",
    "    # Extracting true labels and predictions\n",
    "    y_true = [class_names.index(true_label) for _, _, _, true_label in results]\n",
    "    y_pred = [class_names.index(pred_label) for _, _, pred_label, _ in results]\n",
    "    \n",
    "    # Checking for missing predictions\n",
    "    missing_predictions = sum(1 for pred in y_pred if pred not in range(len(class_names)))\n",
    "\n",
    "\n",
    "    # Computing confusion matrix\n",
    "    cm_matrix = sk_confusion_matrix(y_true, y_pred)  # Use sklearn's confusion_matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)  # Use a blue color map for the plot\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Suppressing warnings for undefined metrics\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "\n",
    "    # Overall Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Printing number of missing predictions\n",
    "    print(f\"\\nNumber of missing predictions: {missing_predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abce480-a9ef-4c4a-9ee6-0293d02cdd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix as sk_confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def evaluate_ensemble(results, class_names):\n",
    "    # Extracting true labels and predictions\n",
    "    y_true = [class_names.index(true_label) for _, _, _, true_label in results]\n",
    "    y_pred = [class_names.index(pred_label) for _, _, pred_label, _ in results]\n",
    "\n",
    "    # Checking for missing predictions\n",
    "    missing_predictions = sum(1 for pred in y_pred if pred not in range(len(class_names)))\n",
    "\n",
    "    # Computing confusion matrix\n",
    "    cm_matrix = sk_confusion_matrix(y_true, y_pred)  # Use sklearn's confusion_matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)  # Use a blue color map for the plot\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Suppressing warnings for undefined metrics\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
    "        \n",
    "        # Generating the classification report\n",
    "        report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0, output_dict=True)\n",
    "\n",
    "        # Converting the classification report to a DataFrame for better visualization\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df = report_df.round(4)\n",
    "\n",
    "        # Plotting the classification report as a heatmap with gradient coloring\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(report_df.iloc[:, :-1].astype(float), annot=True, cmap='Blues', fmt='.4f', cbar=True, annot_kws={\"size\": 12})\n",
    "        plt.title(\"Classification Report\", fontsize=16)\n",
    "        plt.tick_params(axis='both', labelsize=14)\n",
    "        plt.show()\n",
    "\n",
    "        # Printing detailed classification report\n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(report_df)\n",
    "\n",
    "    # Overall Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Printing number of missing predictions\n",
    "    print(f\"\\nNumber of missing predictions: {missing_predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e0cc0",
   "metadata": {},
   "source": [
    "### Unweighted Hard Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4451b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating results\n",
    "evaluate_ensemble(hard_results, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66fb93",
   "metadata": {},
   "source": [
    "### Weighted Hard Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a18922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating results\n",
    "evaluate_ensemble(hard_results_weighted, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1d2e6",
   "metadata": {},
   "source": [
    "### Unweighted Soft Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c96fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating results\n",
    "evaluate_ensemble(soft_results, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c53d6",
   "metadata": {},
   "source": [
    "### Weighted Soft Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating results\n",
    "evaluate_ensemble(soft_results_weighted, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
